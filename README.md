# ğŸ“˜ Forwardâ€“Momentum Scaling Law (AAAI 2026)

This repository provides a minimal, self-contained reference implementation of the **Forwardâ€“Momentum Scaling Law** proposed in:

**Scaling and Transferability of Annealing Strategies in Large Language Model Training**  
*AAAI 2026 (Main Track, Poster)*  
Siqi Wang, Zhengyu Chen, Teng Xiao, Zheqi Lv, Jinluan Yang, Xunliang Cai, Jingang Wang, Xiaomeng Li

---

## ğŸ” Overview

This repository implements the key components introduced in our paper, including:

### âœ” **Forward Effect**
Computation of the cumulative *forward learning-rate effect*, characterized by the integral
```math
S_1 = \int \eta(t) \, dt
```


### âœ” **Annealing Momentum**
A practical proxy for the *kinetic effect* of learning-rate decay, defined via a momentum-style update: 
```math
M = \sum_t \frac{m_t}{\sqrt{v_t}+\epsilon}
```
capturing both the **rate** and **magnitude** of decay during annealing.

### âœ” **Forwardâ€“Momentum Scaling Law** 
A unified scaling formulation: 
```math 
L = L_0 + \frac{\lambda_S}{S^{\alpha_S}} + \frac{\lambda_N}{N^{\alpha_N}} + \lambda_M \cdot M
```

### âœ” **Robust Curve Fitting**
Robust Huber-loss optimization with L-BFGS-B for stable estimation of scaling parameters.

---

## ğŸ“‚ Repository Contents
- annealing_scaling_law.py
- README.md


This code is intentionally minimal. It provides only the components required to reproduce the **annealing-related scaling behavior** discussed in the paper.

---

## â–¶ï¸ Usage

```python
from annealing_scaling_law import compute_S1, compute_S2, fit_and_evaluate_lr_mom

S1 = compute_S1(steps, learning_rates)
S2 = compute_S2(steps, learning_rates)

y_fit, r2, mse, params = fit_and_evaluate_lr_mom(
    y=loss_values,
    x=S1,
    t=S2,
    n=model_size,
    initial_params=[...]
)
```



